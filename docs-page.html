<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>CoderDocs - Bootstrap 4 Documentation Template For Software Projects</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="description" content="Bootstrap 4 Template For Software Startups">
    <meta name="author" content="Xiaoying Riley at 3rd Wave Media">    
    <link rel="shortcut icon" href="favicon.ico"> 
    
    <!-- Google Font -->
    <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome JS-->
    <script defer src="assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">

    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="assets/css/theme.css">

</head> 

<body class="docs-page">    
    <header class="header fixed-top">	    
        <div class="branding docs-branding">
            <div class="container-fluid position-relative py-2">
                <div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible mr-2 d-xl-none" type="button">
	                    <span></span>
	                    <span></span>
	                    <span></span>
	                </button>
	                <div class="site-logo"><a class="navbar-brand" href="index.html"><img class="logo-icon mr-2" src="assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Sunbird<span class="text-alt">Docs</span></span></a></div>    
                </div><!--//docs-logo-wrapper-->
	            <div class="docs-top-utilities d-flex justify-content-end align-items-center">
	                <div class="top-search-box d-none d-lg-flex">
		                <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form>
	                </div>
	
					<ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="#"><i class="fab fa-github fa-fw"></i></a></li>
			            
		             
		                <li class="list-inline-item"><a href="#"><i class="fab fa-product-hunt fa-fw"></i></a></li>
		            </ul><!--//social-list-->
		            <a href="https://themes.3rdwavemedia.com/bootstrap-templates/startup/coderdocs-free-bootstrap-4-documentation-template-for-software-projects/" class="btn btn-primary d-none d-lg-flex">Download</a>
	            </div><!--//docs-top-utilities-->
            </div><!--//container-->
        </div><!--//branding-->
    </header><!--//header-->
    
    <div class="docs-wrapper">
	    <div id="docs-sidebar" class="docs-sidebar">
		    <div class="top-search-box d-lg-none p-3">
                <form class="search-form">
		            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
		            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
		        </form>
            </div>
		    <nav id="docs-nav" class="docs-nav navbar">
			    <ul class="section-items list-unstyled nav flex-column pb-3">
				  </i></span>Installation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">Fow Windows Users</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-2">Fow Ubuntu Users</a></li>
				    
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-2-3">Fow Linux Users</a></li>
				    



				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span class="theme-icon-holder mr-2"><i class="fas fa-box"></i></span>Missing Values</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Median Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Mean Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">Mode Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-4">Random Sample Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-5">End of Distribution Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-6">Arbitrary Value Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-7">Capture NAN Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-8">Frequent Value Imputation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-9">Add Extra Variable</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-3-10">Fill With Missing Imputation</a></li>



				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span class="theme-icon-holder mr-2"><i class="fas fa-cogs"></i></span>Outliers</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">flooring_capping Technique</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-2">trimming Technique</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-3">IQR Technique</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-4">logarithemic Technique</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-4-5">median Technique</a></li>
				    
				    



				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-5"><span class="theme-icon-holder mr-2"><i class="fas fa-tools"></i>
				    </span>Categorical Encoding</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-1">Frequency Encoding</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-2">Target Guided Encoding</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-3">Mean Encoding</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-4">Probability Ratio Encoding</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-5">One Hot Encoding</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-5-6">One Hot With Multi Categories Encoding</a></li>
				    



				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-6"><span class="theme-icon-holder mr-2"><i class="fas fa-laptop-code"></i></span>Feature Selection</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-6-1">SelectBest</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-6-2">RemoveCollinearity</a></li>
				    



				    <li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-7"><span class="theme-icon-holder mr-2"><i class="fas fa-tablet-alt"></i></span>Normalization and Standardization</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-7-1">Gaussian Transformation</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-7-2">MinMaxScaler</a></li>
				    <li class="nav-item"><a class="nav-link scrollto" href="#item-7-3">StandardScaler</a></li>
				   
			    </ul>
		    </nav><!--//docs-nav-->
	    </div><!--//docs-sidebar-->
	    <div class="docs-content">
		    <div class="container">
	
			    
			    <article class="docs-article" id="section-2">
				    <header class="docs-header">
					    <h1 class="docs-heading">Installation</h1>
					    <section class="docs-intro">
						    <p>How to install  sunbird?.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-2-1">
						<h2 class="section-heading">For Windows User</h2>
						<p>Windows users can download the sunbird library from the below given command using commond prompt.</p>
						<input type="text" value="pip install sunbird" disabled>
					</section><!--//section-->
					<section class="docs-section" id="item-2-2">
						<h2 class="section-heading">For Ubuntu Users</h2>
						<p>Ubuntu users can download the sunbird library from the below given command using terminal.</p>
						<input type="" value="pip3 install sunbird" disabled>
					</section><!--//section-->
					
					<section class="docs-section" id="item-2-3">
						<h2 class="section-heading">For Linux Users</h2>
						<p>Linux users can download the sunbird library from the below given command using terminal</p>
						<input type="" value="pip3 install sunbird" disabled>
					</section><!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-3">
				    <header class="docs-header">
					    <h1 class="docs-heading">Missing Values</h1>
					    <section class="docs-intro">
						    <p>Datasets may have missing values, and this can cause problems for many machine learning algorithms. As such, it is good practice to identify and replace missing values for each column in your input data prior to modeling your prediction task. This is called missing data imputation, or imputing for short.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Median Imputation</h2>
						<p>Columns in the dataset which are having numeric continuous values can be replaced with the mean, median, or mode of remaining values in the column. This method can prevent the loss of data compared to the earlier method. Replacing the above two approximations (mean, median) is a statistical approach to handle the missing values.</p>
					</section>
					<!--//section-->

							<!-- ** Embed github code starts ** -->
							<script src="https://gist.github.com/Vaishnavi2392000/69518e41822fa6ddeed7f3b1373492e3.js"></script>
							
					
					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Mean Imputation</h2>
						<p>Columns in the dataset which are having numeric continuous values can be replaced with the mean, median, or mode of remaining values in the column. This method can prevent the loss of data compared to the earlier method. Replacing the above two approximations (mean, median) is a statistical approach to handle the missing values.</p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/1e194c304281a7793f74a089a7388f8c.js"></script>

					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">Mode Imputation</h2>
						<p>Columns in the dataset which are having numeric continuous values can be replaced with the mean, median, or mode of remaining values in the column. This method can prevent the loss of data compared to the earlier method. Replacing the above two approximations (mean, median) is a statistical approach to handle the missing values.</p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/30abd6c1d1d91e8cb9e055f76e129382.js"></script>

					<section class="docs-section" id="item-3-4">
						<h2 class="section-heading">Random Sample Imputation</h2>
						<p>Random sampling imputation consists of extracting random observations from the pool of available values in the variable. Random sampling imputation preserves the original distribution, which differs from the other imputation techniques.</p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/c6f0e3888b28570fd5662098f6cbde13.js"></script>

					<section class="docs-section" id="item-3-5">
						<h2 class="section-heading">End Of Distribution Imputation</h2>
						<p>If there is suspicion that the missing value is not at random then capturing that information is important. In this scenario, one would want to replace missing data with values that are at the tails of the distribution of the variable.</p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/d14ef0fcd1c10ff10af177d8ba3ceaf9.js"></script>

					<section class="docs-section" id="item-3-6">
						<h2 class="section-heading">Arbitrary Value Imputation</h2>
						<p>Arbitrary value imputation consists of replacing all occurrences of missing values within a variable by an arbitrary value. Ideally arbitrary value should be different from the median/mean/mode, and not within the normal values of the variable</p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/b7ba6b0e07abb6685a5bd464f265e213.js"></script>

					<section class="docs-section" id="item-3-7">
						<h2 class="section-heading">Capture NAN Imputation</h2>
						<p>Capuring nan imputation sugegst that this feature engineering technique is basicly used on the data which is not missing completely at random.This technique is used for capturing importance of missing values.But due to adding additinal features it may lead to Curse of dimensionality.</p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/074f53e5374baa7231e06f0365e64e9b.js"></script>

					<section class="docs-section" id="item-3-8">
						<h2 class="section-heading">Frequent Value Imputation</h2>
						<p>Frequent value imputation is used for handling catagarical missing values. In this technique we sort the values in the dataset according to their ascending order.The first value in that order become the <b>most frequent value</b> and we repalce other missing values with that perticular <b>most frequent value</b></p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/3a0f0423cd10d2f77a09e8c3c04d4225.js"></script>

					<section class="docs-section" id="item-3-9">
						<h2 class="section-heading">Adding Extra Variable Imputation</h2>
						<p>Adding extra variable this method is used when we have large amount of missing values.In this feature engineering technique first we replace all missing valeus with 1 and others reamin as it is.Then by finding out the the most frequent value we fill the missing value with that most frequent value </p>
					</section>
					<script src="https://gist.github.com/Vaishnavi2392000/15031c6f57d5d2ee301cb3ed075ba29f.js"></script>


					<section class="docs-section" id="item-3-10">
						<h2 class="section-heading">Fill With Missing Imputation</h2>
						<p.>This is the simplest feature engineering technique that is used when we a lot of missing values.In this technique we replace missing value with <b>Missing</b>wordby keeping remaining values as it is.</p>
					</section><!--//section-->
					<script src="https://gist.github.com/Vaishnavi2392000/2041dc894a17bde562a502a974dad435.js"></script>
			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-4">
				    <header class="docs-header">
					    <h1 class="docs-heading">Outliers</h1>
					    <section class="docs-intro">
						    <p>Section intro goes here. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque finibus condimentum nisl id vulputate. Praesent aliquet varius eros interdum suscipit. Donec eu purus sed nibh convallis bibendum quis vitae turpis. Duis vestibulum diam lorem, vitae dapibus nibh facilisis a. Fusce in malesuada odio.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-4-1">
						<h2 class="section-heading">flooring_capping Technique</h2>
						<p>In this technique, we will do the flooring (e.g., the 10th percentile) for the lower values and capping (e.g., the 90th percentile) for the higher values. These values will be used for quantile-based flooring and capping..</p>
					</section><!--//section-->
					<script src="https://gist.github.com/Vaishnavi2392000/9784c7ab82daa47c5c4dee58bec81537.js"></script>
					
					<section class="docs-section" id="item-4-2">
						<h2 class="section-heading">Trimming Technique</h2>
						<p>In this method, we completely remove data points that are outliers. It creates an index for all the data points where any feature  takes these two values. Then we  drop these index rows from the data, and finally we  summararize statistics for the variable.
						 minimum and maximum values are much more acceptable..</p>
					</section><!--//section-->
					<script src="https://gist.github.com/Vaishnavi2392000/89a5709eaabdc6d3fedae42ea625bc99.js"></script>
					
					<section class="docs-section" id="item-4-3">
						<h2 class="section-heading">InterQuartile Range Technique.</h2>
						<p>This technique uses the IQR Technique scores calculated  to remove outliers. The rule of thumb is that anything not in the range of (Q1 - 1.5 IQR Technique) and (Q3 + 1.5 IQR Technique) is an outlier, and can be removed. It removes outliers based on the IQR Technique range . This shows that for our data, a lot of records get deleted if we use the IQR Technique method.</p>
					</section><!--//section-->
					<script src="https://gist.github.com/Vaishnavi2392000/932715f0224b8224c35dbb18abdee13b.js"></script>

					<section class="docs-section" id="item-4-4">
						<h2 class="section-heading">logarithem.</h2>
						<p>Transformation of the skewed variables may also help correct the distribution of the variables. These could be logarithmic, square root, or square transformations. The most common is the logarithmic transformation.</p>
					</section><!--//section-->
					<script src="https://gist.github.com/Vaishnavi2392000/020ac86d521d6bdc21e119b5862b7254.js"></script>



					<section class="docs-section" id="item-4-5">
						<h2 class="section-heading">median Technique.</h2>
						<p>In this technique, we replace the extreme values with median values. It is advised to not use mean values as they are affected by outliers. It cretes the 50th percentile value, or the median, which comes out to be 140. Then we create 95th percentile value, Then line of code below replaces all those values which are greater than the 95th percentile, with the median value. Finally,  Finally summary statistics after all these techniques have been employed for outlier treatment.</p>
					</section><!--//section-->
					<script src="https://gist.github.com/Vaishnavi2392000/6a849310ae446818e941a7225a872774.js"></script>




			    </article><!--//docs-article-->
			    
			    <article class="docs-article" id="section-5">
				    <header class="docs-header">
					    <h1 class="docs-heading">Categorical Encoding</h1>
					    <section class="docs-intro">
						    <p>Categorical Feature Encoding Techniques

Categorical data is a common type of non-numerical data that contains label values and not numbers. Some examples include: Colors: Red, Green, Blue. Cities: New York, Austin, Denver. Gender: Male, Female.</p>
						</section>
						<!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-5-1">
						<h2 class="section-heading">Frequency Encoding</h2>
						<p>It is a way to utilize the frequency of the categories as labels. In the cases where the frequency is related somewhat with the target variable, it helps the model to understand and assign the weight in direct and inverse proportion, depending on the nature of the data.</p>
					</section><!--//section-->
						
						<div class="docs-code-block">
							<!-- ** Embed github code starts ** -->
							<script src="https://gist.github.com/vivekpisal/0af7e74de7f6b78bffa03dfd69f5209b.js"></script>
							<img src="assets/images/frequency.png">
					
					<section class="docs-section" id="item-5-2">
						<h2 class="section-heading">Target Guided Encoding</h2>
						<p>features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.</p>
					</section><!--//section-->
						
						<script src="https://gist.github.com/vivekpisal/be9eae44471d486f816783ad878a8905.js"></script>
						<img src="assets/images/target.png">

					<section class="docs-section" id="item-5-3">
							<h2 class="section-heading">Mean Encoding</h2>
						<p>Mean Encoding or Target Encoding is one viral encoding approach followed by Kagglers. There are many variations of this. Here I will cover the basic version and smoothing version. Mean encoding is similar to label encoding, except here labels are correlated directly with the target. For example, in mean target encoding for each category in the feature label is decided with the mean value of the target variable on a training data. This encoding method brings out the relation between similar categories, but the connections are bounded within the categories and target itself. The advantages of the mean target encoding are that it does not affect the volume of the data and helps in faster learning. Usually, Mean encoding is notorious for over-fitting; thus, a regularization with cross-validation or some other approach is a must on most occasions.</p>
						
					</section>
							<script src="https://gist.github.com/vivekpisal/29cc2aa1710947c3d279696b31ca7a9f.js"></script>

							<img src="https://miro.medium.com/max/700/1*iiM9g-qCa-Vff_HAFk-ppQ.png">

					<section class="docs-section" id="item-5-4">
						<h2 class="section-heading">Probability Ratio Encoding</h2>
						<p>Probability Ratio Encoding is similar to Weight Of Evidence(WoE), with the only difference is the only ratio of good and bad probability is used. For each label, we calculate the mean of target=1, that is the probability of being 1 ( P(1) ), and also the probability of the target=0 ( P(0) ). And then, we calculate the ratio P(1)/P(0) and replace the labels by that ratio. We need to add a minimal value with P(0) to avoid any divide by zero scenarios where for any particular category, there is no target=0.</p>
					</section>

					<script src="https://gist.github.com/vivekpisal/58b3b1ad460768f8aa2aa366e40a5c68.js"></script>

					<img src="assets/images/probabilty.png">
					<section class="docs-section" id="item-5-5">
						<h2 class="section-heading">One Hot Encoding</h2>
						<p>In this method, we map each category to a vector that contains 1 and 0 denoting the presence or absence of the feature. The number of vectors depends on the number of categories for features. This method produces a lot of columns that slows down the learning significantly if the number of the category is very high for the feature.</p>
					</section>


					<script src="https://gist.github.com/vivekpisal/34ee2945f6481be7ac42dd626961d05a.js"></script>

					<img src="assets/images/onehot.png">	

					<section class="docs-section" id="item-5-6">
						<h2 class="section-heading">One Hot Encoding With Multi Categories</h2>
						<p>In the winning solution of the KDD 2009 cup: "Winning the KDD Cup Orange Challenge with Ensemble Selection" <a href="http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf">http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf</a>, the authors limit one hot encoding to the 10 most frequent labels of the variable. This means that they would make one binary variable for each of the 10 most frequent labels only. This is equivalent to grouping all the other labels under a new category, that in this case will be dropped. Thus, the 10 new dummy variables indicate if one of the 10 most frequent labels is present (1) or not (0) for a particular observation.</p>
					</section>
					<script src="https://gist.github.com/vivekpisal/fc2ec3cf4f718b6a38c09c474e78aaaf.js"></script><!--//section-->
			    </article><!--//docs-article-->
			    
			    
		        <article class="docs-article" id="section-6">
				    <header class="docs-header">
					    <h1 class="docs-heading">Feature Selection</h1>
					    <section class="docs-intro">
						    <p>In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features for use in model construction.</p>
						</section><!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-6-1">
						<h2 class="section-heading">SelectBest</h2>
						<p>It takes as a parameter a score function, which must be applicable to a pair (X, y). ... So, for example, if you pass chi2 as a score function, <b>SelectBest</b> will compute the chi2 statistic between each feature of X and y (assumed to be class labels). A small value will mean the feature is independent of y.</p>
					</section><!--//section-->
						<script src="https://gist.github.com/vivekpisal/db5f8e8e54c65d4ddbef2d1846df96df.js"></script>

					<section class="docs-section" id="item-6-2">
						<h2 class="section-heading">RemoveCollinearity</h2>
						<p>In statistics, multicollinearity (also collinearity) is a phenomenon in which one feature variable in a regression model is highly linearly correlated with another feature variable. A collinearity is a special case when two or more variables are exactly correlated.</p>
					</section>
						<script src="https://gist.github.com/vivekpisal/87e4c10a90f362359e4ca6ef03c3020a.js"></script>
					<!--//section-->
			    </article><!--//docs-article-->
			    
			    
			    <article class="docs-article" id="section-7">
				    <header class="docs-header">
					    <h1 class="docs-heading">Normalization and Standardization</h1>
					    <section class="docs-intro">
						    <p>The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1.</p>
						</section>
						<!--//docs-intro-->
				    </header>
				     <section class="docs-section" id="item-7-1">
						<h2 class="section-heading">Gaussian Transformation</h2>
						<p>Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.</p>
					</section><!--//section-->
						
						<script src="https://gist.github.com/vivekpisal/3e58a7fea3b4b7a5504b49cd36f2c44a.js"></script>

					<section class="docs-section" id="item-7-2">
						<h2 class="section-heading">MinMaxScaler</h2>
						<p>Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.</p>
					</section><!--//section-->
					<script src="https://gist.github.com/vivekpisal/6610f5ea22ac0accfa0efebf12c28aee.js"></script>

					<section class="docs-section" id="item-7-3">
							<h2 class="section-heading">StandardScaler</h2>
						<p>StandardScaler. StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler makes the mean of the distribution 0. About 68% of the values will lie be between -1 and 1.</p>
					</section>
					<script src="https://gist.github.com/vivekpisal/56238ba5eae31b400e1db33820d3446a.js"></script>
					<!--//section-->
			  </article><!--//docs-article-->

			  <footer class="footer">
				    <div class="container text-center py-5">
					    <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
				        <small class="copyright">Designed with <i class="fas fa-heart" style="color: #fb866a;"></i> by <b> Sunbird</b> for developers</small>
				        <br>
            
						    <li class="list-inline-item"><a href="#"><i class="fab fa-github fa-fw"></i></a></li> 
				            
				            <li class="list-inline-item"><a href="#"><i class="fab fa-product-hunt fa-fw"></i></a></li>
				           
				            <li class="list-inline-item"><a href="#"><i class="fab fa-instagram fa-fw"></i></a></li>
				        </ul><!--//social-list-->
				    </div>
			    </footer>
		    </div> 
	    </div>
    </div><!--//docs-wrapper-->
    
   
       
    <!-- Javascript -->          
    <script src="assets/plugins/jquery-3.4.1.min.js"></script>
    <script src="assets/plugins/popper.min.js"></script>
    <script src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>  
    
    
    <!-- Page Specific JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
    <script src="assets/js/highlight-custom.js"></script> 
    <script src="assets/plugins/jquery.scrollTo.min.js"></script>
    <script src="assets/plugins/lightbox/dist/ekko-lightbox.min.js"></script> 
    <script src="assets/js/docs.js"></script> 

</body>
</html> 

